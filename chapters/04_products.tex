\chapter{Computing Matrix Products}

\section{The Distance Product}\label{sec:dist-prod}

The imperative aim of this chapter is to make a conceptual jump from computational geomemtry to the graph theoretic shortest paths problem.
To this end, we discuss the distance product and algebraic relations of matrix products.
Turning away from Algebra and towards Graph Theory, we will finally connect the loose ends we have introduced throughout this manuscript.

We start off with the definition of the \emph{min-plus product}, equivalently known as the \emph{distance product} to which we will restrict ourselves.
In~\cite[Chapter~25]{Cormen2001}, the distance product is inherently linked to the canon matrix product by replacement of agebraic operations, whereas~\cite{Chan2007} defines it as a consequence of the structure of APSPPs.
In contrast to these two practically inferred definitions,\ \cite[Section~5.6]{Aho1974} rigourously constructs the distance product in an axiomatic fashion.
We aim for the common ground of both practices: define the distance product mathematically sound, and skim off the relevant algorithmic properties.

\begin{definition}[Distance Product]\label{def:distance-product}
    The distance product $\otimes: R^{m \times n} \times R^{n \times k} \rightarrow R^{m \times k}$ is defined elementwise by
    \[
        \forall A \in R^{m \times n}, B \in R^{n \times k}, i = 1, 2, \dots, m, j = 1, 2, \dots, k: {\left( A \otimes B \right)}_{i, j} \coloneqq \min\limits_{\ell = 1, 2, \dots, n} a_{i, \ell} + b_{\ell, j}.
    \]
\end{definition}

This product actually is a matrix product over a closed semiring on $R$ provided we consider quadratic matrices, a property that will be relevant in Section~\ref{sec:fast-matrix-products}.
In theory,\ \cite[Example~5.9]{Aho1974} only states $R$ to be the set of nonnegative real numbers including $\infty$, but an extension to negative numbers bounded from below by an appropriately chosen constant is possible to allow for more general consideration.
This remark only makes sense in the application to APSPPs when taking into account the assumption that no negative weight cycles may exist.

The importance of this matrix product for solving an APSPP may not be apparent at first.
To clarify, consider the weight update from the Floyd-Warshall algorithm (Figure~\ref{alg:floyd-warshall}):
\[
    w_{i, j} \coloneqq \min(w_{i, j}, w_{i, k} + w_{k, j}).
\]
Rewriting the first value in the minimum as $w_{i, j} = w_{i, i} + w_{i, j}$, or $w_{i, j} = w_{i, j} + w_{j, j}$ respectively, we notice we can rephrase our original relaxation step in terms of the distance product.
This identification of the distance product to the relaxation should give ample explanation for the use of the distance product when tackling APSPPs.

Besides the algorithmic intuition of relaxing weights during runtime using a matrix product, the distance product introduces another property which was already apparent in the algorithms mentioned so far, but does not follow immediately when looking at the abstract Definition~\ref{def:distance-product}.
The \emph{closure} is an abstract operator $*$ acting on elements of a closed semiring in some manner.
In particular, for a closed semiring $S$ with the usual arithmetic operations of addition and multiplication, the closure of an element $a \in S$ is given by $a^* = \sum\limits_{i \in \bb{N}_{0}} a^i$, cf.~\cite[Section~5.6]{Aho1974}.
The existence of such a closure is guaranteed by the inherent axiomatic properties of a closed semiring.
Interestingly, because $A^* = 0$ for the distance product, cf.~\cite[Example~5.10]{Aho1974}, the relaxation step can be directly expressed in terms of the closure, cf~\cite[Section~5.8]{Aho1974}.
Most importantly,\ \cite[Section~5.9, Corollary~2]{Aho1974} connects the computational costs of matrix products to the costs of calculating the closure of a matrix, which we will use in the next section.

\section{Fast Computation of Matrix Products}\label{sec:fast-matrix-products}

With the key to the method proposed in~\cite{Chan2007} in hand, we can now orient ourselves towards applying it in an efficient way, that is solve the question
\begin{displayquote}
    ``How do we quickly multiply two matrices using an underlying structure?''
\end{displayquote}

A very standard technique is \emph{Strassen's Matrix Multiplication Algorithm} (cf.~\cite[Section~6.2]{Aho1974}), which involves a split of both factor matrices into four blocks, and performing a set of algebraic operations on them.
The result is a computation requiring only $\mathcal{O}\left( n^{\log(7)} \right)$ arithmetic operations, cf.~\cite[Theorem~6.1]{Aho1974}.
However, the statement of this theorem poses the problem that this statement generally only holds when both matrices originate from a ring instead of the closed semiring which the distance product induces.
The method used in~\cite{Chan2007} nevertheless tackles the matrix multiplication by first splitting both factor matrices, where the important difference is the way we partition the matrix.
While Strassen relies on creating four submatrices for fast multiplication, our use case of partitioning is the particular reason for introducing the $\frac{1}{\log(n)}$ factor in the total computational time.

A heuristicly motivated consideration suggests that for APSPPs there exists a number $N \in \bb{N}$ such that $\forall k > N: \bigotimes\limits_{i = 1}^k W = \bigotimes\limits_{i = 1}^N W$.
Intuitively, this means the estimated weight matrix of our graph will no longer receive any updates after a certain iteration, which is obviously the case when we have traversed all possible verteces from any one starting vertex within the graph.
When optimizing the number of matrix multiplications performed we can thus deduce \emph{repeated squaring} for $N = 2^{\lceil \log(n - 1) \rceil}$:
\begin{align*}
    W^1 &= W, \\
    W^2 &= W^1 \cdot W^1, \\
    W^4 &= W^2 \cdot W^2, \\
    &\phantom{=}\!\vdots \\
    W^{2^{\lceil \log(n - 1) \rceil}} &= W^{2^{\lceil \log(n - 1) \rceil} - 1} \cdot W^{2^{\lceil \log(n - 1) \rceil} - 1}.
\end{align*}
This approach would do the trick in solving an APSPP, but the connection between computing matrix products and the closure of a matrix mentioned at the end of Section~\ref{sec:dist-prod} will yield a better result.

\begin{theorem}[{Adapted from~\cite[Lemma~3.1]{Chan2007}}]\label{thm:rect-distance-product}
    Let $A \in \bb{R}^{n \times d}, B \in \bb{R}^{d \times n}$ be two matrices, where we understand $R = \bb{R}$ in the sense suitable for the distance product.
    Then $A \otimes B$ can be computed in a time of $\mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + n^2 \right)$.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:rect-distance-product}]
    The idea used in~\cite{Chan2007} exploits the structure of appropriately sorted sets, an idea akin to that presented in~\cite{Dobosiewicz2007}.
    Proving the assertion manifests in two steps:
    \begin{enumerate}
        \item Define the sets and observe the connection to dominating pairs, and
        \item calculate the cardinality of the sets in relation to the matrix sizes.
    \end{enumerate}

    Consider the matrices $A$ and $B$ to have the entries ${(a_{i, k})}_{i, k = 1}^{n, d}$ and ${(b_{k, j})}_{k, j = 1}^{d, n}$ respectively.
    We start off by defining the sets of index pairs for each $k = 1, 2, \dots, d$
    \[
        X_k \coloneqq \iset{(i, j)}{\forall k' = 1, 2, \dots, d: a_{i, k} + b_{k, j} \leq a_{i, k'} + b_{k', j}} \subseteq \sset{1, 2, \dots, n} \times \sset{1, 2, \dots, n}.
    \]
    By simple arithmetic manipulation we obtain the equivalence
    \begin{equation}\label{eq:dominating-pair-inequality}
        a_{i, k} + b_{k, j} \leq a_{i, k'} + b_{k', j} \iff a_{i, k} - a_{i, k'} \leq b_{k', j} - b_{k, j}.
    \end{equation}
    The right hand term portraits the connection to dominating pairs: for a fixed $k \in \sset{1, 2, \dots, d}$ consider the vectors $\fk{a}_i \coloneqq {(a_{i, k} - a_{i, k'})}_{k' = 1}^d$ and $\fk{b}_j \coloneqq {(b_{k', j} - b_{k, j})}_{k' = 1}^d$.
    Therefore, computing the set $X_k$ for some fixed $k \in \sset{1, 2, \dots, d}$ is equivalent to finding all dominating pairs between the sets $\bb{P}_{red}^{(k)} = {\{ \fk{a}_i \}}_{i = 1}^n$ and $\bb{P}_{blue}^{(k)} = {\{ \fk{b}_j \}}_{j = 1}^n$.
    Lemma~\ref{lem:dominating-pairs} thus gives a computational complexity of $\mathcal{O}\left( c_\varepsilon^d n^{1 + \varepsilon} + \abs{X_k} \right)$.
    Summation over all indices $k = 1, 2, \dots, d$, the total time effort is equal to
    \[
        \sum\limits_{k = 1}^d \mathcal{O}\left( c_\varepsilon^d n^{1 + \varepsilon} + \abs{X_k} \right) = \mathcal{O}\left( \sum\limits_{k = 1}^d \left( c_\varepsilon^d n^{1 + \varepsilon} + \abs{X_k} \right) \right) = \mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + \sum\limits_{k = 1}^d \abs{X_k} \right).
    \]

    Compared to the proposition we only need to account for the sum $\sum\limits_{k = 1}^d \abs{X_k}$.
    Taken literally, we want to count all index pairs and see there are exactly $n^2$ of them.
    First, we need to make clear how we actually evaluate the ``$\leq$'' comparison when computing the dominating pairs.
    In the worst case, it may hold that $a_{i, k} + b_{k, j} = a_{i, k'} + b_{k', j}$ for some $k' \in \sset{1, 2, \dots, d}$.
    In an effort to reduce ambiguity of the minima, we apply the following tie-breaker:
    \begin{displayquote}
        If the inequality in~\ref{eq:dominating-pair-inequality} is not sharp, that is $\exists k' \in \sset{1, 2, \dots, n}: a_{i, k} + b_{k, j} = a_{i, k'} + b_{k', j}$, then we treat $a_{i, k} + b_{k, j}$ as smaller than $a_{i, k'} + b_{k', j}$ if $k$ is smaller than $k'$.
    \end{displayquote}
    We will prove that every index pair $(i, j) \in \sset{1, 2, \dots, n}^2$ is contained exactly once in $\bigcup\limits_{k = 1}^d X_k$:
    \begin{enumerate}
        \item %
            Suppose an index pair $(i, j)$ is included in two different sets $X_k, X_{\tilde{k}}$ for $k \neq \tilde{k}$.
            Then by definition of $X_k$, it holds that $\forall k' = 1, 2, \dots, d: a_{i, k} + b_{k, j} \leq a_{i, k'} + b_{k', j}$.
            Especially, the inequality $a_{i, k} + b_{k, j} \leq a_{i, \tilde{k}} + b_{\tilde{k}, j}$ is true due to the special case $k' = \tilde{k}$.
            Analogously, by the definition of $X_{\tilde{k}}$ we get $\forall k' = 1, 2, \dots, d: a_{i, \tilde{k}} + b_{\tilde{k}, j} \leq a_{i, k'} + b_{k', j}$, as well as $a_{i, \tilde{k}} + b_{\tilde{k}, j} \leq a_{i, k} + b_{k, j}$ for $k' = k$.
            Both inequalities combined yield $a_{i, k} + b_{k, j} = a_{i, \tilde{k}} + b_{\tilde{k}, j}$, placing us firmly in the use case of the tie-breaking mechanism.
            W.l.o.g.\ we can assume $k < \tilde{k}$, wherefore the tie-breaker considers the equality as ``$a_{i, k} + b_{k, j} < a_{i, \tilde{k}} + b_{\tilde{k}, j}$''.
            This concludes that $(i, j) \not\in X_{\tilde{k}}$. \Lightning{}
        \item %
            Assume there exists some $(i, j)$ for which it holds $\forall k = 1, 2, \dots, d: (i, j) \not\in X_k$.
            The pair can thus be characterized by the condition $\forall k = 1, 2, \dots, d: \exists k' = 1, 2, \dots, d: a_{i, k} + b_{k, j} > a_{i, k'} + b_{k', j}$.
            Because the set $\sset{1, 2, \dots, d}$ is finite we can now choose $\tilde{k} \coloneqq \argmin\limits_{m = 1, 2, \dots, d} a_{i, m} + b_{m, j}$.
            The index $\tilde{k}$ thus satisfies $\forall k' = 1, 2, \dots, d: a_{i, \tilde{k}} + b_{\tilde{k}, j} \leq a_{i, k'} + b_{k', j}$, meaning that the index pair $(i, j)$ would have been included in $X_{\tilde{k}}$ in contradiction to the assumption. \Lightning{}
    \end{enumerate}
    In conclusion, it holds $\bigcup\limits_{k = 1}^d X_k = \sset{1, 2, \dots, d}^2 \implies \mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + \sum\limits_{k = 1}^d \abs{X_k} \right) = \mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + n^2 \right)$.
\end{proof}

Conceptually, we can now quickly multiply rectangular matrices.
It is thus advantageous to split two original factor matrices $A, B \in \bb{R}^{n \times n}$ into $\frac{n}{\ell} \in \bb{N}$ rectangular blocks (allowing for some slack to compensate for rounding errors) $A_i \in \bb{R}^{n \times \ell}, B_i \in \bb{R}^{\ell \times n}$ for $i = 1, 2, \dots, \frac{n}{\ell}$ such that $A$ and $B$ can be reconstructed as $A = \begin{pmatrix}
        A_1^T, A_2^T, \dots, A_{\frac{n}{\ell}}^T
    \end{pmatrix},
    B = \begin{pmatrix}
        B_1, B_2, \dots, B_{\frac{n}{\ell}}
    \end{pmatrix}$.
In the particular case of the distance product we can very easily multiply two rectangular blocks because of the following statement.

\begin{lemma}[Computing Split Distance Products]\label{lem:split-distance-products}
    Let $A$ and $B$ be matrices split into rectangular blocks as described above.
    Then the distance product of $A$ and $B$ is defined elementwise as
    \[
        \forall i, j = 1, 2, \dots, n: {(A \otimes B)}_{i, j} = \min\limits_{m = 1, 2, \dots, \frac{n}{\ell}} {(A_m \otimes B_m)}_{i, j}.
    \]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:split-distance-products}]
    We denote the entries of the blocks $A_m$ and $B_m$ for each $m = 1, 2, \dots, \frac{n}{\ell}$ by $a_{i, k}^{(m)}$ and $b_{k, j}^{(m)}$.
    By applying the definition of the distance product it follows that
    \[
        {(A \otimes B)}_{i, j} \overset{def.}{=} \min\limits_{k = 1, 2, \dots, n} a_{i, k} + b_{k, j} = \min\limits_{m = 1, 2, \dots, \frac{n}{\ell}} \left( \min\limits_{k = 1, 2, \dots, \ell} a_{i, k}^{(m)} + b_{k, j}^{(m)} \right) \overset{def.}{=} \min\limits_{m = 1, 2, \dots, \frac{n}{\ell}} {(A_m \otimes B_m)}_{i, j},
    \]
    where the inner equation follows from the fact that the matrix split also partitions the indices of all matrix entries, which means that the first minimum exactly corresponds to the latter two minima.
\end{proof}

All that is left to do now is to bundle up all results and lay down how they apply to APSPPs in the end.
We begin by incorporating the split distance product computation from Lemma~\ref{lem:split-distance-products} into the result from Theorem~\ref{thm:rect-distance-product}.
Afterwards we finish by portraiting the runtime for the APSPP-solving algorithm.

\begin{theorem}[{Adapted from~\cite[Theorem~3.2]{Chan2007}}]\label{thm:distance-product}
    Let $A, B \in \bb{R}^{n \times n}$ be two matrices.
    Then we can compute their distance product in $\mathcal{O}\left( \frac{n^3}{\log(n)} \right)$ time.
\end{theorem}

\begin{proof}[Proof ot Theorem~\ref{thm:distance-product}]
    Let $\ell \in \bb{N}$ be given; we will discuss the choice of this constant later in the proof.
    Firstly, employ the matrix split discussed before Lemma~\ref{lem:split-distance-products}: for $m = 1, 2, \dots, \frac{n}{\ell}$ let $A_m \in \bb{R}^{n \times \ell}$ denote the rectangular blocks of $A$, and $B_m \in \bb{R}^{\ell \times n}$ those of $B$ respectively.
    Secondly, we compute the rectangular distance products $A_m \otimes B_m$ for all $m = 1, 2, \dots, \frac{n}{\ell}$ according to Theorem~\ref{thm:rect-distance-product}.
    The operation $A_m \otimes B_m$ needs to be carried out $\frac{n}{\ell}$ times, totaling up to a time effort of $\mathcal{O}\left( \frac{n}{\ell} \left( \ell c_\varepsilon^\ell n^{1 + \varepsilon} \right) \right) = \mathcal{O}\left( c_\varepsilon^\ell n^{2 + \varepsilon} + \frac{n^3}{\ell} \right)$.
    Afterwards, the quadratic distance product $A \otimes B$ is calculated as detailed in Lemma~\ref{lem:split-distance-products}, which incurs a computational cost of $\mathcal{O}\left( \frac{n}{\ell} n^2 \right) = \mathcal{O}\left( \frac{n^3}{\ell} \right)$ because we need to take the minimum over $\frac{n}{\ell}$ matrices of size $n \times n$.
    In combination, both steps thus amount to $\mathcal{O}\left( c_\varepsilon^\ell n^{2 + \varepsilon} + \frac{n^3}{\ell} \right)$ time.
    To finish the proof we make a few observations with respect to the constant $\ell$:
    \begin{itemize}
        \item %
            Choosing the obvious option $\ell \sim n$ gives us a nice term of $n^2$ in the $\frac{n^3}{\ell}$ part of the cost, the drawback however comes in the other summand.
            We compute that $c_\varepsilon^n n^{2 + \varepsilon}$ grows exponentially, and will thus dominate the polynomial behaviour of $n^2$ in the long run.
            Therefore, the naive choice results in a runtime worse than Floyd-Warshall.
            Even less trivial variants like $\ell \sim n^{1 - \varepsilon}$ retain the exponential growth: $c_\varepsilon^{n^{1 - \varepsilon}}$, and are thus unsuited.
        \item %
            Checking the growth by picking a factor such that $\ell \sim C \log(n)$ for some sufficiently small constant $C \in \bb{R}_{> 0}$ on the other hand delivers our desired result.
            We finally verify
            \[
                \mathcal{O}\left( c_\varepsilon^{C \log(n)} + \frac{n^3}{C \log(n)} \right) = \mathcal{O}\left( n^{2 + C + \varepsilon} + \frac{n^3}{\log(n)} \right) = \mathcal{O}\left( \frac{n^3}{\log(n)} \right).
            \]
    \end{itemize}
    This finishes the proof.
\end{proof}

\begin{corollary}[{Adapted from~\cite[Corollary~3.3]{Chan2007}}]\label{cor:apsp-subcubic}
    The All Pairs Shortest Paths Problem is solvable with a computational effort in the order of $\mathcal{O}\left( \frac{n^3}{\log(n)} \right)$ time.
\end{corollary}

\begin{proof}[Proof of Corollary~\ref{cor:apsp-subcubic}]
    qwer
\end{proof}

\section{Recovering Shortest Paths}\label{sec:recovery}

All the way back in Section~\ref{sec:apspps} we mentioned that the recovery of the shortest paths found by the method described in~\cite{Chan2007} takes a fundamentally different form than in the cases of less involved approaches such as the generalization of Bellman-Ford or Floyd-Warshall.
Whereas both more traditional methods as well as the idea discussed here involve a comparison related to the relaxation step, we can recover all shortest paths much more easily from the sets we constructed in Theorem~\ref{thm:rect-distance-product}.
Noting that the tie-breaker introduced in the proof of Theorem~\ref{thm:rect-distance-product} ensures that the minima in the inequality used to define the $X_k, k = 1, 2, \dots, n$, are unique, we argue as follows:

Firstly, the case where we seek a shortest path from a vertex $u \in V$, indexed by $i$ in $\bb{N}$, to itself can be trivially resolved.
It is always true that $\forall k' = 1, 2, \dots, n: w_{i, i} + w_{i, i} \leq w_{i, k'} + w_{k', i}$ because otherwise a negative weight cycle would exist in the graph.
Thus, the index pair $(i, i)$ will always be included in the set $X_i$.
Highlighting the converse implication is unnecessary to reach the following equivalence:
\begin{displayquote}
    ``Iff there are no negative weight cycles, then $(i, i) \in X_i$ holds for all $i$.''
\end{displayquote}

Secondly, if we consider two neighbouring vertices indexed in $V$ by the natural numbers $i$ and $j$, such that $i \neq j$ and w.l.o.g.\ $i < j$, where the shortest path from vertex $i$ to vertex $j$ is the connecting edge $(i, j) \in E$, then the inequality $w_{i, i} + w_{i, j} \leq w_{i, k'} + w_{k', j}$ trivially holds for all $k' = 1, 2, \dots, n$.
Therefore, the index pair $(i, j)$ is included in the set $X_i$.
Vice versa, assuming again w.l.o.g.\ $i < j$, if it holds that $(i, j) \in X_i$, then the shortest path from vertex $i$ to vertex $j$ must consist of the direct edge $(i, j) \in E$.
By defining $m \in \bb{N}$ to resolve our assumed ordering $i < j$, that is $m \coloneqq \min \{ i, j \}$, we have thusly observed an equivalence of shortest path existence and set inclusion:
\begin{displayquote}
    ``Iff neighbouring vertices have a direct shortest path, then $(i, j) \in X_m$.''
\end{displayquote}

Lastly, consider the more general case that the index pair $(i, j)$ is included in the set $X_k$ for some $k \in \sset{1, 2, \dots, n}$.
By the usual inequality $\forall k' = 1, 2, \dots, n: w_{i, k} + w_{k, j} \leq w_{i, k'} + w_{k', j}$ we observe that any shortest path originating in the vertex $i \in V$ leading to the vertex $j \in V$ must pass through the intermediary $k \in V$.
The converse analogously implies that if there is a shortest path between $i$ and $j$ which passes through $k$, then it must hold that $(i, j) \in X_k$.
We thus arrive at the equivalence:
\begin{displayquote}
    ``Iff a shortest path from $i \in V$ to $j \in V$ passes through the vertex $j$, then $(i, j) \in X_k$.''
\end{displayquote}

These statements in combination give an idea how to reconstruct any existing shortest path between arbitrary vertices $i$ and $j$ in $V$ expressed in Figure~\ref{alg:recovery}.
This algorithm has the trivial termination criterion that all $n^2$ possible index combinations have been concatenated into a single path, leaving no more possible edges to consider.

\begin{figure}[ht]
    \centering
    \begin{minipage}{.9\textwidth}
        \begin{algorithm}[H]
            \KwData{
                Index sets $X_k$ for $k = 1, 2, \dots, n$, starting vertex $i \in V$, target vertex $j \in V$
            }
            \SetKwProg{Def}{def}{:}{}
            \Def{get\_shortest\_path$(i, j)$}{
                Set $k$ such that $(i, j) \in X_k$\;
                \If{$k \not\in \{ i, j \}$}{
                    Set $\mathfrak{p} := $ \emph{get\_shortest\_path}$(i, k)$ $\oplus$ \emph{get\_shortest\_path}$(k, j)$\;
                }
                \Else{
                    Set $\mathfrak{p} := (i, j)$\;
                }
                \Return{$\mathfrak{p}$}
            }
        \end{algorithm}
    \end{minipage}
    \caption{Recovery of shortest paths}\label{alg:recovery}
\end{figure}
