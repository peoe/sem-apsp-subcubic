\section{Connecting Computational Geometry and Matrix Multiplication}

\begin{frame}{Basic Ideas}
    The proof is structured as follows:
    \begin{enumerate}
        \item<1-> A problem in computational geometry $\implies$ \emph{Finding dominating pairs}
        \item<2-> Computing products of matrices $\implies$ \emph{Sorting indices in sets}
        \item<3-> Combining the two ideas $\implies$ \emph{Multiplying matrices efficiently}
    \end{enumerate}

    \begin{figure}
        \begin{tikzpicture}[scale=.55]
            \node[barlabel] (compgeom) at (0, 3) {CG};
            \uncover<2->{
                \node[barlabel] (matmul) at (3, 3) {MM};
                \path[dominates] (compgeom) -- (matmul);
            }
            \uncover<3->{
                \node[barlabel] (apsp) at (3, 0) {APSP};
                \path[dominates] (matmul) -- (apsp);
            }
            \uncover<4->{
                \path[dominates] (compgeom) -- (apsp);
            }
        \end{tikzpicture}
    \end{figure}
\end{frame}

\begin{frame}{Dominating Pairs\only<1>{\footnote[1]{\cite{Chan2007}}}}
    \begin{alertblock}{Dominating Pairs}
        Let $\mathbb{P}_{red}, \mathbb{P}_{blue}$ be (finite) subsets of $\mathbb{R}^d$ with $d \in \mathbb{N}$.
        Then a \emph{dominating pair} is defined to be a tuple $(p, q) \in \mathbb{P}_{red} \times \mathbb{P}_{blue}$ such that it holds
        \[
            \forall k = 1, \dots, d: p_k \leq q_k,
        \]
        where $p_k$, and $q_k$ are the $k$-th coordinate of $p$, and $q$ respectively.
    \end{alertblock}
    \uncover<2->{
        \begin{block}{Notes}
            The choice of the names ``red'' and ``blue'' is arbitrary.

            \uncover<3>{
                This implies that for any dominating pair, the first point will always be the one with smaller coordinates.
                This can, to a certain degree, be seen as an ``ordering'' of $\mathbb{P}_{red}$, and $\mathbb{P}_{blue}$.
            }
        \end{block}
    }
\end{frame}

\begin{frame}{Dominating Pairs}
    \begin{figure}
        \begin{tikzpicture}[scale=0.75, auto, swap]
            \foreach \pos / \name in {%
                {(1, 1)/a}, {(4.5, 3.5)/b}%
            }%
                \node[redpoint] (\name) at \pos {};

            \foreach \pos / \name in {%
                {(3, 0.5)/A}, {(2, 3)/B}, {(5, 5)/C}%
            }%
                \node[bluepoint] (\name) at \pos {};

            \only<5->{
                \foreach \source / \dest in {%
                    B/a, C/a,%
                    C/b%
                }%
                    \path[dominates] (\source) -- (\dest);
            }

            \node[barlabel] (labelx) at (3, -0.75) {Coordinate 2};
            \node[barlabel, rotate=90] (labely) at (-0.75, 3) {Coordinate 1};

            \node[ghost] (bound1) at (0, 0) {};
            \node[ghost] (bound2) at (6, 6) {};

            % pareto fronts
            \begin{pgfonlayer}{background}
                \uncover<2>{\node[paretofront, fit=(bound1) (A.center)] (paretoA) {};}
                \uncover<3>{\node[paretofront, fit=(bound1) (B.center)] (paretoB) {};}
                \uncover<4>{\node[paretofront, fit=(bound1) (C.center)] (paretoC) {};}
            \end{pgfonlayer}

            % border
            \node[draw, fit=(bound1) (bound2)] (border) {};
        \end{tikzpicture}
    \end{figure}
\end{frame}

\begin{frame}{Lemma 2.1\footnote[1]{\cite{Chan2007}}}
    \begin{lemma}\label{lem:dom_pairs}
        Let $\mathbb{P} \subset \mathbb{R}^d$ be a set of red or blue points, partitioned into $\mathbb{P}_{red}$ and $\mathbb{P}_{blue}$ such that $n = \abs{\mathbb{P}_{red}} + \abs{\mathbb{P}_{blue}}$, and $\varepsilon \in \interval[open]{0}{1}$.
        Then we can find all $k$ dominating pairs in a time of $\mathcal{O}\left( c_\varepsilon^d n^{1 + \varepsilon} + k \right)$, where we define $c_\varepsilon := \frac{2^\varepsilon}{2^\varepsilon - 1}$.
    \end{lemma}
\end{frame}

\begin{frame}{Matrix Products}
    \begin{alertblock}{Min-Plus/Distance Product\only<1>{\footnote[1]{\cite{Chan2007}}}}
        We define the \emph{min-plus} or \emph{distance product} as the operator
        \[
            \otimes : \mathbb{R}^{n \times d} \times \mathbb{R}^{d \times m} \rightarrow \mathbb{R}^{n \times m},\qquad
            {\left( A \otimes B \right)}_{i, j} := \min\limits_{k \in \{ 1, \dots, d \}} a_{i, k} + b_{k, j}.
        \]
    \end{alertblock}

    \uncover<2->{
        \begin{columns}
            \begin{column}{.53\linewidth}
                \begin{exampleblock}{Computing Distances}
                    This product is useful, because it mimics the computation of shortest distances:
                    \begin{figure}
                        \begin{minipage}{.8\linewidth}
                            \begin{algorithm}[H]
                                \If{$d(a, c) > d(a, b) + d(b, c)$}{
                                    $d(a, c) := d(a, b) + d(b, c)$
                                }
                            \end{algorithm}
                        \end{minipage}
                    \end{figure}
                \end{exampleblock}
            \end{column}
            \begin{column}{.4\linewidth}
                \begin{figure}
                    \begin{tikzpicture}[scale=0.95, auto, swap]
                        \foreach \pos / \name in {%
                            {(0, 0)/a}, {(5, 0)/c}, {(2.5, 2)/b}%
                        }%
                            \node[vertex] (\name) at \pos {$\name$};
            
                        \foreach \source / \dest / \weight in {%
                            a/b/3, a/c/7, b/c/2%
                        }%
                            \path[edge] (\source) -- node[weight] {$\weight$} (\dest);

                        \foreach \vertex / \fr in {a/3, c/3}%
                            \path<\fr-> node[selected vertex] at (\vertex) {$\vertex$};

                        \foreach \vertex / \fr in {a/4, b/4, c/4}%
                            \path<\fr-> node[selected vertex] at (\vertex) {$\vertex$};

                        \only<3>{
                            \begin{pgfonlayer}{background}
                                \path<+->[selected edge] (a.center) -- (c.center);
                            \end{pgfonlayer}
                        }

                        \only<4>{
                            \begin{pgfonlayer}{background}
                                \foreach \source / \dest in {a/b, b/c}
                                    \path<+->[selected edge] (\source.center) -- (\dest.center);
                            \end{pgfonlayer}
                        }
                    \end{tikzpicture}
                \end{figure}
            \end{column}
        \end{columns}
    }
\end{frame}

\begin{frame}{Matrix Products}
    \begin{alertblock}{Min-Plus/Distance Product}
        We define the \emph{min-plus} or \emph{distance product} as the operator
        \[
            \otimes : \mathbb{R}^{n \times d} \times \mathbb{R}^{d \times m} \rightarrow \mathbb{R}^{n \times m},\qquad
            {\left( A \otimes B \right)}_{i, j} := \min\limits_{k \in \{ 1, \dots, d \}} a_{i, k} + b_{k, j}.
        \]
    \end{alertblock}

    \begin{figure}
        \begin{tikzpicture}[auto, swap]
            \foreach \pos / \name in {%
                {(0, 0)/a}, {(2, 2)/b}, {(5, 2)/c}, {(7, 0)/d}%
            }%
                \node[vertex] (\name) at \pos {$\name$};

            \foreach \source / \dest / \weight in {%
                a/b/3, b/c/4, c/d/2%
            }%
                \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
            
            \only<2-5>{
                \begin{pgfonlayer}{background}
                    \path<+->[selected dashed edge] (a.center) -- node[weight] {?} (d.center);
                \end{pgfonlayer}
            }

            \only<3>{
                \begin{pgfonlayer}{background}
                    \foreach \source / \dest in {a/b, b/c}
                        \path<+->[selected edge] (\source.center) -- (\dest.center);
                \end{pgfonlayer}
            }

            \only<4->{
                \path[dashed edge] (a) -- node[weight] {$7$} (c);
            }

            \only<5>{
                \begin{pgfonlayer}{background}
                    \foreach \source / \dest in {a/c, c/d}
                        \path<+->[selected edge] (\source.center) -- (\dest.center);
                \end{pgfonlayer}
            }

            \only<7>{
                \path[dashed edge] (a) -- node[weight] {$9$} (d);
            }
        \end{tikzpicture}
    \end{figure}
\end{frame}

\begin{frame}{Lemma 3.1\footnote[1]{\cite{Chan2007}}}
    \begin{lemma}\label{lem:sub_mat_mul}
        Given two matrices $A \in \mathbb{R}^{n \times d}, B \in \mathbb{R}^{d \times n}$, and $\varepsilon \in \interval[open]{0}{1}, \begin{aligned}c_\varepsilon := \frac{2^\varepsilon}{2^\varepsilon - 1}\end{aligned}$, we can compute their min-plus product $A \otimes B$ in $\mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + n^2 \right)$ time.

        Here, $\varepsilon$ and $c_\varepsilon$ are the same variables as in Lemma~\ref{lem:dom_pairs}.
    \end{lemma}
\end{frame}

\begin{frame}{Splitting Quadratic Matrices}
    \textbf{Q:} How do we go about multiplying two matrices efficiently?

    \uncover<2->{\textbf{A:} Split and multiply individually!\only<2->{ (Strassen's method\only<2>{\footnote[1]{\cite[Section~6.2]{Aho1974}}})}}

    \uncover<3->{
        \begin{columns}
            \begin{column}{.45\linewidth}
                \begin{align*}
                    C &= A B \\
                    \uncover<4->{
                        \begin{pmatrix}
                            C_1 & C_2 \\
                            C_3 & C_4
                        \end{pmatrix}
                        &=
                        \begin{pmatrix}
                            A_1 & A_2 \\
                            A_3 & A_4
                        \end{pmatrix}
                        \begin{pmatrix}
                            B_1 & B_2 \\
                            B_3 & B_4
                        \end{pmatrix}\\
                    }
                \end{align*}
            \end{column}
            \begin{column}{.5\linewidth}
                \[
                    \uncover<5->{
                        \implies 
                        \left\{
                            \begin{aligned}
                                C_1 &= A_1 B_1 + A_2 B_3 \\
                                C_2 &= A_1 B_2 + A_2 B_4 \\
                                C_3 &= A_3 B_1 + A_4 B_3 \\
                                C_4 &= A_3 B_2 + A_4 B_4 \\
                            \end{aligned}
                        \right.
                    }
                \]
            \end{column}
        \end{columns}
    }
\end{frame}

\begin{frame}{Splitting Quadratic Matrices}
    \textbf{Q:} How do we go about multiplying two matrices efficiently?

    \textbf{A:} Split and multiply individually! (Strassen's method)
    
    \begin{align*}
        C &= A B \uncover<2->{&&M_1 = \left( A_2 - A_4 \right) \left( B_3 + B_4 \right)} \\
        \begin{pmatrix}
            C_1 & C_2 \\
            C_3 & C_4
        \end{pmatrix}
        &=
        \begin{pmatrix}
            A_1 & A_2 \\
            A_3 & A_4
        \end{pmatrix}
        \begin{pmatrix}
            B_1 & B_2 \\
            B_3 & B_4
        \end{pmatrix} \uncover<2->{&&M_2 = \left( A_1 + A_4 \right) \left( B_1 + B_4 \right)} \\
        \uncover<2->{
            C_1 &= M_1 + M_2 - M_4 + M_6 &&M_3 = \left( A_1 - A_3 \right) \left( B_1 + B_2 \right) \\
            C_2 &= M_4 + M_5 &&M_4 = \left( A_1 + A_2 \right) B_4 \\
            C_3 &= M_6 + M_7 &&M_5 = A_1 \left( B_2 - B_3 \right) \\
            C_4 &= M_2 - M_3 + M_5 - M_7 &&M_6 = A_4 \left( B_3 - B_1 \right) \\
            &\phantom{{}} &&M_7 = \left( A_3 + A_4 \right) B_1 \\
        }
    \end{align*}
\end{frame}

\begin{frame}{Multipyling Matrices and Computing their Closure}
    \begin{alertblock}{Problem}
        Strassen's method does not work for closed semirings.\only<1>{\footnote[1]{\cite{Chan2007}}}
        However we want to work with the min-plus product which induces a closed semiring.\only<1>{\footnote[2]{\cite[Example~5.9]{Aho1974}}}
    \end{alertblock}

    \uncover<2->{
        We can instead make use of the closure of the matrix defined as $\begin{aligned}[t]A^* := \sum\limits_{i = 0}^\infty A^i\end{aligned}$.
    }

    \uncover<3->{
        It can then be shown that under some conditions on the computational time needed to multiply two square matrices and the time needed to compute the closure of a square matrix, both times are of the same order.\only<3>{\footnote[1]{\cite[Section~5.9]{Aho1974}}}
    }

    \uncover<4>{
        This can then be related to the APSP problem via the closure under the min-plus product.\only<4>{\footnote[1]{\cite[Section~5.9, Corollary~2]{Aho1974}}}
    }
\end{frame}

\begin{frame}{Multiplying Matrices\footnote[1]{\cite{Chan2007}}}
    We will make use of splitting matrices into intermediaries whilst avoiding Strassen's method (although they ressemble each other closely):
    \begin{columns}
        \begin{column}{.45\linewidth}
            \begin{align*}
                A =
                \begin{tikzpicture}[scale=.9, baseline]
                    \matrix[hsupermatrix]{
                        \node[vsubmatrix] (a1) {A_1}; \& \node[vsubmatrix] (a2) {A_2}; \& \cdots \& \node[vsubmatrix] (ad) {A_d}; \\
                    };
                \end{tikzpicture}
            \end{align*}
        \end{column}
        \begin{column}{.45\linewidth}
            \begin{align*}
                B =
                \begin{tikzpicture}[scale=.9, baseline]
                    \matrix[vsupermatrix]{
                        \node[hsubmatrix] (b1) {B_1}; \\ \node[hsubmatrix] (b2) {B_2}; \\ \vdots \\ \node[hsubmatrix] (bd) {B_d}; \\
                    };
                \end{tikzpicture}
            \end{align*}
        \end{column}
    \end{columns}

    $\begin{aligned}
        \implies \left( C = A \otimes B \iff c_{i, j} = \min\limits_{m = 1, \dots, d} {\left( A_m \otimes B_m \right)}_{i, j} \right)
    \end{aligned}$
\end{frame}

\begin{frame}{Theorem 3.2\footnote[1]{\cite{Chan2007}} \only<2>{\& Corollary 3.3\footnotemark[1]}}
    \begin{theorem}\label{thm:mat_mul}
        Given any two matrices $A, B \in \mathbb{R}^{n \times n}$ we can compute their min-plus (distance) product in a time of $\mathcal{O}\left( n^3 / \log(n) \right)$.
    \end{theorem}

    \uncover<2>{
        \begin{corollary}\label{cor:apsp_subcubic}
            We can solve the all pairs shortest paths problem for a graph $G = (V, E)$ with $\abs{V} = n$ vertices in $\mathcal{O}\left( n^3 / \log(n) \right)$ time.
        \end{corollary}
    }
\end{frame}
