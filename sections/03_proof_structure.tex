\section{Connecting Computational Geometry and Matrix Multiplication}

\begin{frame}{Basic Ideas}
    The proof is structured as follows:
    \begin{enumerate}
        \item<1-> A problem in computational geometry $\implies$ \emph{Finding dominating pairs}
        \item<2-> Computing products of matrices $\implies$ \emph{Sorting indices in sets}
        \item<3-> Combining the two ideas $\implies$ \emph{Multiplying matrices efficiently}
    \end{enumerate}

    \uncover<4->{
        \begin{figure}
            \begin{tikzpicture}[scale=.55]
                \node[barlabel] (compgeom) at (0, 3) {CG};
                \uncover<5->{
                    \node[barlabel] (matmul) at (3, 3) {MM};
                    \path[dominates] (compgeom) -- (matmul);
                }
                \uncover<6->{
                    \node[barlabel] (apsp) at (3, 0) {APSP};
                    \path[dominates] (matmul) -- (apsp);
                }
                \uncover<7->{
                    \path[dominates] (compgeom) -- (apsp);
                }
            \end{tikzpicture}
        \end{figure}
    }
\end{frame}

\begin{frame}{Dominating Pairs\only<1>{\footnote[1]{\cite{Chan2007}}}}
    \begin{alertblock}{Dominating Pairs}
        Let $\mathbb{P}_{red}, \mathbb{P}_{blue}$ be (finite) subsets of $\mathbb{R}^d$ with $d \in \mathbb{N}$.
        Then a \emph{dominating pair} is defined to be a tuple $(p, q) \in \mathfrak{M}_1 \times \mathfrak{M}_2$ such that it holds
        \[
            \forall k = 1, \dots, d: p_k \leq q_k,
        \]
        where $p_k$, and $q_k$ are the $k$-th coordinate of $p$, and $q$ respectively.
    \end{alertblock}
    \uncover<2->{
        \begin{block}{Notes}
            The choice of the names ``red'' and ``blue'' is arbitrary.

            \uncover<3>{
                This implies that for any dominating pair, the first point will always be the one with smaller coordinates.
                This can, to a certain degree, be seen as an ``ordering'' of $\mathbb{P}_{red}$, and $\mathbb{P}_{blue}$.
            }
        \end{block}
    }
\end{frame}

\begin{frame}{Dominating Pairs}
    \begin{figure}
        \begin{tikzpicture}[scale=0.75, auto, swap]
            \foreach \pos / \name in {%
                {(1, 1)/a}, {(4.5, 3.5)/b}%
            }%
                \node[redpoint] (\name) at \pos {};

            \foreach \pos / \name in {%
                {(3, 0.5)/A}, {(2, 3)/B}, {(5, 5)/C}%
            }%
                \node[bluepoint] (\name) at \pos {};

            \only<2>{
                \foreach \source / \dest in {%
                    B/a, C/a,%
                    C/b%
                }%
                    \path[dominates] (\source) -- (\dest);
            }

            \node[barlabel] (labelx) at (3, -0.75) {Coordinate 2};
            \node[barlabel, rotate=90] (labely) at (-0.75, 3) {Coordinate 1};

            \node[ghost] (bound1) at (0, 0) {};
            \node[ghost] (bound2) at (6, 6) {};

            % border
            \node[draw,fit=(bound1) (bound2)] (border) {};
        \end{tikzpicture}
    \end{figure}
\end{frame}

\begin{frame}{Lemma 2.1\footnote[1]{\cite{Chan2007}}}
    \begin{lemma}\label{lem:dom_pairs}
        Let $\mathbb{P} \subset \mathbb{R}^d$ be a set of red or blue points, partitioned into $\mathbb{P}_{red}$ and $\mathbb{P}_{blue}$ such that $n = \abs{\mathbb{P}_{red}} + \abs{\mathbb{P}_{blue}}$, and $\varepsilon \in \interval[open]{0}{1}$.
        Then we can find all $k$ dominating pairs in a time of $\mathcal{O}\left( c_\varepsilon^d n^{1 + \varepsilon} + k \right)$, where we define $c_\varepsilon := \frac{2^\varepsilon}{2^\varepsilon - 1}$.
    \end{lemma}
\end{frame}

\begin{frame}{Matrix Products}
    \begin{alertblock}{Min-Plus/Distance Product\only<1>{\footnote[1]{\cite{Chan2007}}}}
        We define the \emph{min-plus} or \emph{distance product} as the operator
        \[
            \otimes : \mathbb{R}^{n \times d} \times \mathbb{R}^{d \times m} \rightarrow \mathbb{R}^{n \times m},\qquad
            {\left( A \otimes B \right)}_{i, j} := \min\limits_{k \in \{ 1, \dots, d \}} a_{i, k} + b_{k, j}.
        \]
    \end{alertblock}

    \uncover<2->{
        \begin{columns}
            \begin{column}{.53\linewidth}
                \begin{exampleblock}{Computing Distances}
                    This product is useful, because it mimics the computation of shortest distances:
                    \begin{figure}
                        \begin{minipage}{.8\linewidth}
                            \begin{algorithm}[H]
                                \If{$d(a, c) > d(a, b) + d(b, c)$}{
                                    $d(a, c) := d(a, b) + d(b, c)$
                                }
                            \end{algorithm}
                        \end{minipage}
                    \end{figure}
                \end{exampleblock}
            \end{column}
            \begin{column}{.4\linewidth}
                \begin{figure}
                    \begin{tikzpicture}[scale=0.95, auto, swap]
                        \foreach \pos / \name in {%
                            {(0, 0)/a}, {(5, 0)/c}, {(2.5, 2)/b}%
                        }%
                            \node[vertex] (\name) at \pos {$\name$};
            
                        \foreach \source / \dest / \weight in {%
                            a/b/3, a/c/7, b/c/2%
                        }%
                            \path[edge] (\source) -- node[weight] {$\weight$} (\dest);

                        \foreach \vertex / \fr in {a/3, c/3}%
                            \path<\fr-> node[selected vertex] at (\vertex) {$\vertex$};

                        \foreach \vertex / \fr in {a/4, b/4, c/4}%
                            \path<\fr-> node[selected vertex] at (\vertex) {$\vertex$};

                        \only<3>{
                            \begin{pgfonlayer}{background}
                                \path<+->[selected edge] (a.center) -- (c.center);
                            \end{pgfonlayer}
                        }

                        \only<4>{
                            \begin{pgfonlayer}{background}
                                \foreach \source / \dest in {a/b, b/c}
                                    \path<+->[selected edge] (\source.center) -- (\dest.center);
                            \end{pgfonlayer}
                        }
                    \end{tikzpicture}
                \end{figure}
            \end{column}
        \end{columns}
    }
\end{frame}

\begin{frame}{Matrix Products}
    \begin{alertblock}{Min-Plus/Distance Product\only<1>{\footnote[1]{\cite{Chan2007}}}}
        We define the \emph{min-plus} or \emph{distance product} as the operator
        \[
            \otimes : \mathbb{R}^{n \times d} \times \mathbb{R}^{d \times m} \rightarrow \mathbb{R}^{n \times m},\qquad
            {\left( A \otimes B \right)}_{i, j} := \min\limits_{k \in \{ 1, \dots, d \}} a_{i, k} + b_{k, j}.
        \]
    \end{alertblock}

    \uncover<2->{
        \begin{figure}
            \begin{tikzpicture}[auto, swap]
                \foreach \pos / \name in {%
                    {(0, 0)/a}, {(2, 2)/b}, {(5, 2)/c}, {(7, 0)/d}%
                }%
                    \node[vertex] (\name) at \pos {$\name$};

                \foreach \source / \dest / \weight in {%
                    a/b/3, b/c/4, c/d/2%
                }%
                    \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
                
                \only<3-6>{
                    \begin{pgfonlayer}{background}
                        \path<+->[selected dashed edge] (a.center) -- node[weight] {?} (d.center);
                    \end{pgfonlayer}
                }

                \only<4>{
                    \begin{pgfonlayer}{background}
                        \foreach \source / \dest in {a/b, b/c}
                            \path<+->[selected edge] (\source.center) -- (\dest.center);
                    \end{pgfonlayer}
                }

                \only<5->{
                    \path[dashed edge] (a) -- node[weight] {$7$} (c);
                }

                \only<6>{
                    \begin{pgfonlayer}{background}
                        \foreach \source / \dest in {a/c, c/d}
                            \path<+->[selected edge] (\source.center) -- (\dest.center);
                    \end{pgfonlayer}
                }

                \only<7>{
                    \path[dashed edge] (a) -- node[weight] {$9$} (d);
                }
            \end{tikzpicture}
        \end{figure}
    }
\end{frame}

\begin{frame}{Lemma 3.1\footnote[1]{\cite{Chan2007}}}
    \begin{lemma}\label{lem:sub_mat_mul}
        Given two matrices $A \in \mathbb{R}^{n \times d}, B \in \mathbb{R}^{d \times n}$, and $\varepsilon \in \interval[open]{0}{1}, \begin{aligned}c_\varepsilon := \frac{2^\varepsilon}{2^\varepsilon - 1}\end{aligned}$, we can compute their min-plus product $A \otimes B$ in $\mathcal{O}\left( d c_\varepsilon^d n^{1 + \varepsilon} + n^2 \right)$ time.

        Here, $\varepsilon$ and $c_\varepsilon$ are the same variables as in Lemma~\ref{lem:dom_pairs}.
    \end{lemma}
\end{frame}

\begin{frame}{Splitting Quadratic Matrices}
    \textbf{Q:} How do we go about multiplying two matrices efficiently?

    \uncover<2->{\textbf{A:} Split and multiply individually!\only<2->{ (Strassen's Method\only<2>{\footnote[1]{\cite[Section~6.2]{Aho1974}}})}}

    \uncover<3->{
        \begin{columns}
            \begin{column}{.45\linewidth}
                \begin{align*}
                    C &= A B \\
                    \uncover<4->{
                        \begin{pmatrix}
                            C_1 & C_2 \\
                            C_3 & C_4
                        \end{pmatrix}
                        &=
                        \begin{pmatrix}
                            A_1 & A_2 \\
                            A_3 & A_4
                        \end{pmatrix}
                        \begin{pmatrix}
                            B_1 & B_2 \\
                            B_3 & B_4
                        \end{pmatrix}\\
                    }
                \end{align*}
            \end{column}
            \begin{column}{.5\linewidth}
                \uncover<5->{
                        $\implies 
                        \left\{
                            \begin{aligned}
                                C_1 &= A_1 B_1 + A_2 B_3 \\
                                C_2 &= A_1 B_2 + A_2 B_4 \\
                                C_3 &= A_3 B_1 + A_4 B_3 \\
                                C_4 &= A_3 B_2 + A_4 B_4 \\
                            \end{aligned}
                        \right.$
                }
            \end{column}
        \end{columns}
    }
\end{frame}

\begin{frame}{Multipyling Matrices and Computing their Closure}
    \begin{alertblock}{Problem}
        Strassen's method does not work for closed semirings.\only<1>{\footnote[1]{\cite{Chan2007}}}
        However we want to work with the min-plus product which induces a closed semiring.\only<1>{\footnote[2]{\cite[Example~5.9]{Aho1974}}}
    \end{alertblock}

    \uncover<2->{
        We can instead make use of the closure of the matrix defined as $\begin{aligned}[t]A^* := \sum\limits_{i = 0}^\infty A^i\end{aligned}$.
    }

    \uncover<3->{
        It can then be shown that under some conditions on the computational time needed to multiply two square matrices and the time needed to compute the closure of a square matrix, both times are of the same order.\only<3>{\footnote[1]{\cite[Section~5.9]{Aho1974}}}
    }

    \uncover<4>{
        This can then be related to the APSP problem via the closure under the min-plus product.\only<4>{\footnote[1]{\cite[Section~5.9, Corollary~2]{Aho1974}}}
    }
\end{frame}

\begin{frame}{Multiplying Matrices\footnote[1]{\cite{Chan2007}}}
    We will make use of splitting matrices into intermediaries whilst avoiding Strassen's method (although they ressemble each other closely):
    \begin{columns}
        \begin{column}{.45\linewidth}
            \begin{align*}
                A =
                \begin{tikzpicture}[scale=.9, baseline]
                    \matrix[hsupermatrix]{
                        \node[vsubmatrix] (a1) {A_1}; \& \node[vsubmatrix] (a2) {A_2}; \& \cdots \& \node[vsubmatrix] (ad) {A_d}; \\
                    };
                \end{tikzpicture}
            \end{align*}
        \end{column}
        \begin{column}{.45\linewidth}
            \begin{align*}
                B =
                \begin{tikzpicture}[scale=.9, baseline]
                    \matrix[vsupermatrix]{
                        \node[hsubmatrix] (b1) {B_1}; \\ \node[hsubmatrix] (b2) {B_2}; \\ \vdots \\ \node[hsubmatrix] (bd) {B_d}; \\
                    };
                \end{tikzpicture}
            \end{align*}
        \end{column}
    \end{columns}

    $\begin{aligned}
        \implies \left( C = A \otimes B \iff c_{i, j} = \min\limits_{l = 1, \dots, d} {\left( A_l \otimes B_l \right)}_{i, j} \right)
    \end{aligned}$
\end{frame}

\begin{frame}{Theorem 3.2\footnote[1]{\cite{Chan2007}} \only<2>{\& Corollary 3.3\footnotemark[1]}}
    \begin{theorem}\label{thm:mat_mul}
        Given any two matrices $A, B \in \mathbb{R}^{n \times n}$ we can compute their min-plus (distance) product in a time of $\mathcal{O}\left( n^3 / \log(n) \right)$.
    \end{theorem}

    \uncover<2>{
        \begin{corollary}\label{cor:apsp_subcubic}
            We can solve the all pairs shortest paths problem for a graph $G = (V, E)$ with $\abs{V} = n$ vertices in $\mathcal{O}\left( n^3 / \log(n) \right)$ time.
        \end{corollary}
    }
\end{frame}
